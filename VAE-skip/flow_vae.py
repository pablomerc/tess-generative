# -*- coding: utf-8 -*-
"""flow VAE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swTtVe_mpYEL9I_S_DEPryZtxkvAshO_

## Variational Autoencoders with Flows

First, we load the relevant packages:
"""

import torch; torch.manual_seed(0) #set for reproducibility!
import torch.nn as nn
import torch.nn.functional as F
import torch.utils
import torch.distributions
import torchvision
import numpy as np
import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200
import seaborn as sns

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

data = torch.utils.data.DataLoader(
        torchvision.datasets.MNIST('./data',
               transform=torchvision.transforms.ToTensor(),
               download=True),
        batch_size=128,
        shuffle=True)

"""We can see that the various MNIST digits cluster but the latent space is not well sampled! This leads to problems down the road sampling outside of the training data distribution. We can already see this a bit when interpolating over the space:"""

class VariationalAutoencoder(nn.Module):
    def __init__(self, latent_dims):
        super(VariationalAutoencoder, self).__init__()
        self.enc1 = nn.Linear(784, 512)
        self.enc2 = nn.Linear(512, latent_dims)
        self.enc3 = nn.Linear(512, latent_dims)

        self.dec1 = nn.Linear(latent_dims, 512)
        self.dec2 = nn.Linear(512, 784)

        self.N = torch.distributions.Normal(0, 1)
        self.N.loc = self.N.loc
        self.N.scale = self.N.scale
        self.kl = 0

    def encoder(self, x):
        x = torch.flatten(x, start_dim=1)
        x = F.relu(self.enc1(x))
        mu =  self.enc2(x)
        sigma = torch.exp(self.enc3(x))
        eps = self.N.sample(mu.shape).to(mu.device)
        z = mu + sigma*eps
        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()
        return z

    def decoder(self, z):
        z = F.relu(self.dec1(z))
        z = torch.sigmoid(self.dec2(z))
        return z.reshape((-1, 1, 28, 28))

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

"""We again train for 20 epochs, this time having as our objective the ELBO loss combining reconstruction error and KL-divergence between the latent distribution and a multivariate Gaussian."""

num_epochs = 20
latent_dims = 2 #set 2 as the dimensionality of the bottleneck to visualize results!

VAE_model = VariationalAutoencoder(latent_dims).to(device)

opt = torch.optim.Adam(VAE_model.parameters())
for epoch in range(num_epochs):
    for x, y in data:
        x = x.to(device) # GPU
        opt.zero_grad()
        x_hat = VAE_model(x)
        loss = ((x - x_hat)**2).sum() + VAE_model.kl #objective now the reconstruction loss AND KL-divergence
        loss.backward()
        opt.step()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

def plot_reconstructed(autoencoder, r0=(-5, 10), r1=(-10, 5), n=12):
    w = 28
    img = np.zeros((n*w, n*w))
    for i, y in enumerate(np.linspace(*r1, n)):
        for j, x in enumerate(np.linspace(*r0, n)):
            z = torch.Tensor([[x, y]]).to(device)
            x_hat = autoencoder.decoder(z)
            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()
            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat
    plt.imshow(img, extent=[*r0, *r1])

def plot_latent_vars(model, data, num_batches=100):
    for i, (x, y) in enumerate(data):
        z = model.encoder(x.to(device))
        z = z.to('cpu').detach().numpy()
        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10', ec='k', s=5, lw=0.3)
        if i > num_batches:
            plt.colorbar()
            break
        plt.xlabel("Latent 1")
        plt.ylabel("Latent 2")

plot_latent_vars(VAE_model, data)

"""We can see that we have _regularized_ our latent space: the training data instances better cover the latent space, improving our ability to interpolate through it with unseen data. Try to interpret these distributions! Does the middle clumping make intuitive sense? (do 8's tend to look like 3's tend to look like 5's?)

We can again check how the reconstructions look:
"""

plot_reconstructed(VAE_model, r0=(-3, 3), r1=(-3, 3))

class AffineCoupling(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.net = nn.Sequential(
            nn.Linear(dim // 2, 128),
            nn.ReLU(),
            nn.Linear(128, dim)
        )

    def forward(self, z):
        z1, z2 = z.chunk(2, dim=1)
        h = self.net(z1)
        shift, log_scale = h.chunk(2, dim=1)
        z2 = z2 * torch.exp(log_scale) + shift
        log_det = log_scale.sum(dim=1)
        return torch.cat([z1, z2], dim=1), log_det

    def inverse(self, z):
        z1, z2 = z.chunk(2, dim=1)
        h = self.net(z1)
        shift, log_scale = h.chunk(2, dim=1)
        z2 = (z2 - shift) * torch.exp(-log_scale)
        return torch.cat([z1, z2], dim=1)

class VAEWithFlow(nn.Module):
    def __init__(self, latent_dims, flow_steps=2):
        super().__init__()
        self.enc1 = nn.Linear(784, 512)
        self.enc2 = nn.Linear(512, latent_dims)  # mu
        self.enc3 = nn.Linear(512, latent_dims)  # log_sigma

        self.dec1 = nn.Linear(latent_dims, 512)
        self.dec2 = nn.Linear(512, 784)

        self.base_dist = torch.distributions.Normal(0, 1)
        self.flow = nn.ModuleList([AffineCoupling(latent_dims) for _ in range(flow_steps)])
        self.kl = 0

    def encoder(self, x):
        x = torch.flatten(x, start_dim=1)
        h = F.relu(self.enc1(x))
        mu = self.enc2(h)
        log_sigma = self.enc3(h)
        sigma = torch.exp(log_sigma)

        # Sample from base Gaussian
        eps = self.base_dist.sample(mu.shape).to(mu.device)
        z0 = mu + sigma * eps

        # Flow transformation
        log_det_sum = 0
        z = z0
        for f in self.flow:
            z, log_det = f(z)
            log_det_sum += log_det

        # KL with flow correction
        log_qz0 = (-0.5 * ((z0 - mu) / sigma)**2 - log_sigma - 0.5 * np.log(2 * np.pi)).sum(dim=1)
        log_pz = (-0.5 * z**2 - 0.5 * np.log(2 * np.pi)).sum(dim=1)
        self.kl = (log_qz0 - log_pz - log_det_sum).sum()
        return z

    def decoder(self, z):
        z = F.relu(self.dec1(z))
        z = torch.sigmoid(self.dec2(z))
        return z.reshape((-1, 1, 28, 28))

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

VAE_flow_model = VAEWithFlow(latent_dims=2, flow_steps=4).to(device)
opt_flow = torch.optim.Adam(VAE_flow_model.parameters())
for epoch in range(num_epochs):
    for x, y in data:
        x = x.to(device)
        opt_flow.zero_grad()
        x_hat = VAE_flow_model(x)
        loss = ((x - x_hat)**2).sum() + VAE_flow_model.kl
        loss.backward()
        opt_flow.step()
    print(f'[Flow VAE] Epoch {epoch+1}, Loss: {loss.item():.2f}')

plt.figure(figsize=(6, 6))
plot_latent_vars(VAE_flow_model, data)
plt.title("Latent space colored by digit label")
plt.show()

plt.figure(figsize=(8, 8))
plot_reconstructed(VAE_flow_model, r0=(-5, 5), r1=(-5, 5), n=20)
plt.axis("off")
plt.show()

def plot_recon_samples(model, x, num_samples=5):
    x = x[:1].to(device)  # take one sample only

    with torch.no_grad():
        samples = []
        for _ in range(num_samples):
            output = model(x)
            if isinstance(output, tuple):
                x_hat = output[0]
            else:
                x_hat = output
            samples.append(x_hat.cpu().squeeze(0))

    original = x.cpu().squeeze().numpy()

    plt.figure(figsize=(num_samples + 1, 2))
    plt.subplot(1, num_samples + 1, 1)
    plt.imshow(original)
    plt.title("Original")
    plt.axis('off')

    for i, x_hat in enumerate(samples):
        plt.subplot(1, num_samples + 1, i + 2)
        plt.imshow(x_hat.squeeze())
        plt.title(f"Sample {i+1}")
        plt.axis('off')

    plt.suptitle("Reconstructed samples")
    plt.show()

dataset = data.dataset

x_img, y_label = dataset[42]
plot_recon_samples(VAE_model,x_img.unsqueeze(0))
plot_recon_samples(VAE_flow_model, x_img.unsqueeze(0))
print(y_label)

def one_hot(labels, num_classes=10):
    return F.one_hot(labels, num_classes=num_classes).float()

class ConditionalVAE(nn.Module):
    def __init__(self, latent_dims, num_classes=10):
        super(ConditionalVAE, self).__init__()
        self.latent_dims = latent_dims
        self.num_classes = num_classes
        self.enc1 = nn.Linear(784, 512)
        self.enc2 = nn.Linear(512, latent_dims)
        self.enc3 = nn.Linear(512, latent_dims)

        # Decoder input: latent_dims + num_classes
        self.dec1 = nn.Linear(latent_dims + num_classes, 512)
        self.dec2 = nn.Linear(512, 784)

        self.N = torch.distributions.Normal(0, 1)
        self.N.loc = self.N.loc
        self.N.scale = self.N.scale
        self.kl = 0

    def encoder(self, x):
        x = torch.flatten(x, start_dim=1)
        x = F.relu(self.enc1(x))
        mu =  self.enc2(x)
        sigma = torch.exp(self.enc3(x))
        eps = self.N.sample(mu.shape).to(mu.device)
        z = mu + sigma*eps
        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()
        return z

    def decoder(self, z, y):
        # y is expected to be one-hot
        zy = torch.cat([z, y], dim=1)
        zy = F.relu(self.dec1(zy))
        zy = torch.sigmoid(self.dec2(zy))
        return zy.reshape((-1, 1, 28, 28))

    def forward(self, x, y):
        z = self.encoder(x)
        return self.decoder(z, y)

# Train ConditionalVAE
cond_vae = ConditionalVAE(latent_dims=latent_dims, num_classes=10).to(device)
opt_cond = torch.optim.Adam(cond_vae.parameters())
for epoch in range(num_epochs):
    for x, y in data:
        x = x.to(device)
        y_onehot = one_hot(y, num_classes=10).to(device)
        opt_cond.zero_grad()
        x_hat = cond_vae(x, y_onehot)
        loss = ((x - x_hat)**2).sum() + cond_vae.kl
        loss.backward()
        opt_cond.step()
    print(f'[Conditional VAE] Epoch {epoch+1}, Loss: {loss.item():.4f}')

def plot_latent_vars_cond(model, data, num_batches=100):
    for i, (x, y) in enumerate(data):
        y_onehot = one_hot(y, num_classes=10).to(device)
        z = model.encoder(x.to(device))
        z = z.to('cpu').detach().numpy()
        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10', ec='k', s=5, lw=0.3)
        if i > num_batches:
            plt.colorbar()
            break
        plt.xlabel("Latent 1")
        plt.ylabel("Latent 2")

plt.figure(figsize=(6, 6))
plot_latent_vars_cond(cond_vae, data)
plt.title("Conditional VAE Latent space colored by digit label")
plt.show()

# Plot only the latent space for the original VAE
plt.figure(figsize=(6, 6))
plot_latent_vars(VAE_model, data)
plt.title("VAE Latent space only")
plt.show()

# Plot only the latent space for the ConditionalVAE
plt.figure(figsize=(6, 6))
plot_latent_vars_cond(cond_vae, data)
plt.title("Conditional VAE Latent space only")
plt.show()
